# -*- coding: utf-8 -*-
"""mycopyRobertaFinetunedONDEVDATA_A.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1080G9_vld-EpypoS6rMceg6lKxIRm_54

import the necessary files
"""

import json
import pandas as pd
import numpy as np

"""pandarallel is a simple and efficient tool to parallelize Pandas operations on all available CPUs"""

!pip install pandarallel
from pandarallel import pandarallel
pandarallel.initialize()

"""mounting the gdrive"""

from google.colab import drive
drive.mount('/content/drive')

!gdown --folder https://drive.google.com/drive/folders/11YeloR2eTXcTzdwI04Z-M2QVvIeQAU6-

import shutil

# Source and Destination Paths
source_folder = "/content/SubtaskB"  # Change this to your source folder path
destination_folder = "/content/drive/MyDrive/SubtaskB"  # Change this to your destination folder path

# Copy Files
shutil.copytree(source_folder, destination_folder)

data = []
with open("/content/drive/My Drive/SubtaskA/subtaskA_train_monolingual.jsonl", "r") as file:
    for line in file:
        data.append(json.loads(line))

df = pd.DataFrame(data)

data = []
with open("/content/drive/MyDrive/SubtaskB/subtaskb.jsonl", "r") as file:
    for line in file:
        data.append(json.loads(line))

test = pd.DataFrame(data)

data = []
with open("/content/drive/My Drive/SubtaskB/subtaskB_train.jsonl", "r") as file:
    for line in file:
        data.append(json.loads(line))

dfbtrain = pd.DataFrame(data)

data = []
with open("/content/drive/My Drive/SubtaskB/subtaskB_dev.jsonl", "r") as file:
    for line in file:
        data.append(json.loads(line))

dfbdev = pd.DataFrame(data)

df1 = pd.read_csv("/content/drive/MyDrive/SubtaskB/dfbtrain_enron.csv")



print(test.head(4))

dfbtrain.head(5)

import pandas as pd

# Assuming 'df' is your DataFrame and 'label_column' is the column containing the labels
# For example:
# df = pd.read_csv("your_data.csv")
# label_column = "label"

# Count of labels
label_counts = dfbtrain["label"].value_counts()

print(label_counts)

# Count of unique labels
num_unique_labels = dfbtrain["label"].nunique()

print("Number of unique labels:", num_unique_labels)

len(dfbtrain)

len(dfbdev)

len(test)

print(df.head(4))

source_path = '/content/fine_tuned_model/'  # Colab local file path
destination_path = '/content/drive/My Drive/fine_tuned_model/'  # Google Drive file path

# import shutil
# import os
# # Copy the file from Colab to Google Drive
# files = os.listdir(source_path)
# for file in files:
#     source_name = os.path.join(source_path,file)
#     destination_name = os.path.join(destination_path,file)
#     shutil.copy2(source_name, destination_name)

import pandas as pd
import json

devdata = []
with open("/content/drive/My Drive/SubtaskA/subtaskA_dev_monolingual.jsonl", "r") as file:
    for line in file:
        devdata.append(json.loads(line))

dfdev = pd.DataFrame(devdata)

# Assuming your DataFrame is named df and the text column is named 'text'

# Step 1: Text Cleaning
import re

def clean_text(text):
    # Remove special characters, punctuation, and symbols
    text = re.sub(r'[^\w\s]', '', text)
    # Remove extra whitespace
    text = ' '.join(text.split())
    return text

#df['cleaned_text'] = df['text'].apply(clean_text)
# dfdev['cleaned_text'] = dfdev['text'].apply(clean_text)
# df['cleaned_text'] = df['text'].apply(clean_text)
test['cleaned_text'] = test['text'].apply(clean_text)
test['cleaned_text'] = test['cleaned_text'].str.lower()




# Step 2: Lowercasing
#df['cleaned_text'] = df['cleaned_text'].str.lower()

!pip install transformers

"""importing classifier and tokenizer from the transformers"""

from transformers import RobertaForSequenceClassification, RobertaTokenizer

model1 = RobertaForSequenceClassification.from_pretrained('/content/drive/My Drive/fine_tuned_model/')
tokenizer = RobertaTokenizer.from_pretrained('/content/drive/My Drive/fine_tuned_model/')

# Assuming df_dev contains your dev dataset and 'text' is the column name with text data
dev_texts = train['cleaned_text'].tolist()

# Tokenize the texts
inputs = tokenizer(dev_texts, return_tensors='pt', padding=True, truncation=True,max_length=512)

# Assuming df_dev contains your dev dataset and 'text' is the column name with text data
trained_texts = df['cleaned_text'].tolist()

# Tokenize the texts
inputs1 = tokenizer(trained_texts, return_tensors='pt', padding=True, truncation=True,max_length=512)

import torch

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

!pip install apex

dev_labels = dfdev['label'].tolist()

df_labels = df['label'].tolist()

dev_data = torch.utils.data.TensorDataset(
    torch.tensor(inputs['input_ids']),
    torch.tensor(inputs['attention_mask']),
    torch.tensor(dev_labels)
)

train_data = torch.utils.data.TensorDataset(
    torch.tensor(inputs1['input_ids']),
    torch.tensor(inputs1['attention_mask']),
    torch.tensor(df_labels)
)

!pip uninstall apex
!git clone https://github.com/NVIDIA/apex
!cd apex && python setup.py install

dev_loader = torch.utils.data.DataLoader(dev_data, batch_size=8)

df_loader = torch.utils.data.DataLoader(train_data, batch_size=8)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model1.to(device)

import torch
import pandas as pd

model1.eval()
total_correct = 0
all_predictions = []
all_actual_labels = []

with torch.no_grad():
    for batch in dev_loader:
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        outputs = model1(input_ids=input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=-1)
        total_correct += (predictions == labels).sum().item()

        # Save predictions and actual labels for each batch
        all_predictions.extend(predictions.cpu().numpy())
        all_actual_labels.extend(labels.cpu().numpy())

accuracy = total_correct / len(dev_loader.dataset)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

# Convert predictions and actual labels to a pandas DataFrame
df = pd.DataFrame({'Prediction': all_predictions, 'ActualLabel': all_actual_labels})

# Save DataFrame to Excel
df.to_excel('predictions_with_actual_labels.xlsx', index=False)

import torch
import pandas as pd
import pickle

# Function to get logits from a model
def get_logits(model, dataloader):
    model.eval()
    all_logits = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids, attention_mask, _ = batch
            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits.cpu().numpy()
            all_logits.extend(logits)

    return all_logits

# Get logits for dev data
dev_logits = get_logits(model1, dev_loader)

# Save logits to a pickle file
with open('/content/drive/MyDrive/SubtaskA/dev_logits.pkl', 'wb') as f:
    pickle.dump(dev_logits, f)

# Convert logits to a pandas DataFrame
df_dev_logits = pd.DataFrame(dev_logits, columns=[f'logit_{i}' for i in range(len(dev_logits[0]))])

# Save DataFrame to Excel
df_dev_logits.to_excel('dev_logits_dataframe.xlsx', index=False)


train_logits = get_logits(model1, df_loader)



# Save logits to a pickle file
with open('/content/drive/MyDrive/SubtaskA/train_logits.pkl', 'wb') as f:
    pickle.dump(train_logits, f)

# Convert logits to a pandas DataFrame
df_train_logits = pd.DataFrame(train_logits, columns=[f'logit_{i}' for i in range(len(train_logits[0]))])

# Save DataFrame to Excel
df_train_logits.to_excel('train_logits_dataframe.xlsx', index=False)

# Optionally, get logits from trained data
# train_logits = get_logits(model1, train_loader)
# Save logits from trained data to a pickle file or DataFrame as needed

model1.eval()
total_correct = 0

with torch.no_grad():
    for batch in dev_loader:
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        outputs = model1(input_ids=input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=-1)
        total_correct += (predictions == labels).sum().item()

accuracy = total_correct / len(dev_loader.dataset)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

from string import punctuation
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

def fil_sent(sent):
      """
      Filter stopwords
      """

      filtered_sentence = ' '.join([w for w in sent.split() if not w in stop_words])
      return filtered_sentence

def process(sent):
      """
      Apply stemming
      """

      sent = str(sent)
      return fil_sent(' '.join([ps.stem(str(x).lower()) for x in word_tokenize(sent)]))

stop_words = set(stopwords.words('english'))
ps = PorterStemmer()

def extract_style(text):
    """
    Extracting stylometric features of a text
    """

    text = str(text)
    len_text = len(text)
    len_words = len(text.split())
    avg_len = np.mean([len(t) for t in text.split()])
    num_short_w = len([t for t in text.split() if len(t) < 3])
    per_digit = sum(t.isdigit() for t in text)/len(text)
    per_cap = sum(1 for t in text if t.isupper())/len(text)
    f_a = sum(1 for t in text if t.lower() == "a")/len(text)
    f_b = sum(1 for t in text if t.lower() == "b")/len(text)
    f_c = sum(1 for t in text if t.lower() == "c")/len(text)
    f_d = sum(1 for t in text if t.lower() == "d")/len(text)
    f_e = sum(1 for t in text if t.lower() == "e")/len(text)
    f_f = sum(1 for t in text if t.lower() == "f")/len(text)
    f_g = sum(1 for t in text if t.lower() == "g")/len(text)
    f_h = sum(1 for t in text if t.lower() == "h")/len(text)
    f_i = sum(1 for t in text if t.lower() == "i")/len(text)
    f_j = sum(1 for t in text if t.lower() == "j")/len(text)
    f_k = sum(1 for t in text if t.lower() == "k")/len(text)
    f_l = sum(1 for t in text if t.lower() == "l")/len(text)
    f_m = sum(1 for t in text if t.lower() == "m")/len(text)
    f_n = sum(1 for t in text if t.lower() == "n")/len(text)
    f_o = sum(1 for t in text if t.lower() == "o")/len(text)
    f_p = sum(1 for t in text if t.lower() == "p")/len(text)
    f_q = sum(1 for t in text if t.lower() == "q")/len(text)
    f_r = sum(1 for t in text if t.lower() == "r")/len(text)
    f_s = sum(1 for t in text if t.lower() == "s")/len(text)
    f_t = sum(1 for t in text if t.lower() == "t")/len(text)
    f_u = sum(1 for t in text if t.lower() == "u")/len(text)
    f_v = sum(1 for t in text if t.lower() == "v")/len(text)
    f_w = sum(1 for t in text if t.lower() == "w")/len(text)
    f_x = sum(1 for t in text if t.lower() == "x")/len(text)
    f_y = sum(1 for t in text if t.lower() == "y")/len(text)
    f_z = sum(1 for t in text if t.lower() == "z")/len(text)
    f_1 = sum(1 for t in text if t.lower() == "1")/len(text)
    f_2 = sum(1 for t in text if t.lower() == "2")/len(text)
    f_3 = sum(1 for t in text if t.lower() == "3")/len(text)
    f_4 = sum(1 for t in text if t.lower() == "4")/len(text)
    f_5 = sum(1 for t in text if t.lower() == "5")/len(text)
    f_6 = sum(1 for t in text if t.lower() == "6")/len(text)
    f_7 = sum(1 for t in text if t.lower() == "7")/len(text)
    f_8 = sum(1 for t in text if t.lower() == "8")/len(text)
    f_9 = sum(1 for t in text if t.lower() == "9")/len(text)
    f_0 = sum(1 for t in text if t.lower() == "0")/len(text)
    f_e_0 = sum(1 for t in text if t.lower() == "!")/len(text)
    f_e_1 = sum(1 for t in text if t.lower() == "-")/len(text)
    f_e_2 = sum(1 for t in text if t.lower() == ":")/len(text)
    f_e_3 = sum(1 for t in text if t.lower() == "?")/len(text)
    f_e_4 = sum(1 for t in text if t.lower() == ".")/len(text)
    f_e_5 = sum(1 for t in text if t.lower() == ",")/len(text)
    f_e_6 = sum(1 for t in text if t.lower() == ";")/len(text)
    f_e_7 = sum(1 for t in text if t.lower() == "'")/len(text)
    f_e_8 = sum(1 for t in text if t.lower() == "/")/len(text)
    f_e_9 = sum(1 for t in text if t.lower() == "(")/len(text)
    f_e_10 = sum(1 for t in text if t.lower() == ")")/len(text)
    f_e_11 = sum(1 for t in text if t.lower() == "&")/len(text)
    richness = len(list(set(text.split())))/len(text.split())

    return pd.Series([avg_len, len_text, len_words, num_short_w, per_digit, per_cap, f_a, f_b, f_c, f_d, f_e, f_f, f_g, f_h, f_i, f_j, f_k, f_l, f_m, f_n, f_o, f_p, f_q, f_r, f_s, f_t, f_u, f_v, f_w, f_x, f_y, f_z, f_0, f_1, f_2, f_3, f_4, f_5, f_6, f_7, f_8, f_9, f_e_0, f_e_1, f_e_2, f_e_3, f_e_4, f_e_5, f_e_6, f_e_7, f_e_8, f_e_9, f_e_10, f_e_11, richness])

test['content_tfidf'] = test['text'].parallel_apply(lambda x: process(x))
test[["avg_len", "len_text", "len_words", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9",  "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]] = test['text'].parallel_apply(lambda x: extract_style(x))
test.to_csv("test1.csv", escapechar='\\')

dfbtrain['content_tfidf'] = dfbtrain['text'].parallel_apply(lambda x: process(x))
dfbtrain[["avg_len", "len_text", "len_words", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9",  "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]] = dfbtrain['text'].parallel_apply(lambda x: extract_style(x))
dfbtrain.to_csv("dfbtrain_enron.csv")

dfbdev['content_tfidf'] = dfbdev['text'].parallel_apply(lambda x: process(x))
dfbdev[["avg_len", "len_text", "len_words", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9",  "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]] = dfbdev['text'].parallel_apply(lambda x: extract_style(x))
dfbdev.to_csv("dfbdev_enron.csv")

import shutil

# Source and Destination Paths
source_file = "/content/dfbtrain_enron.csv"  # Change this to your CSV file path
destination_folder = "/content/drive/MyDrive/SubtaskB"  # Change this to your destination folder path

# Copy File
shutil.copy(source_file, destination_folder)

df['content_tfidf'] = df['text'].parallel_apply(lambda x: process(x))
df[["avg_len", "len_text", "len_words", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9",  "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]] = df['text'].parallel_apply(lambda x: extract_style(x))
df.to_csv("full_enron.csv")

test['content_tfidf'] = test['text'].parallel_apply(lambda x: process(x))
test[["avg_len", "len_text", "len_words", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9",  "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]] = test['text'].parallel_apply(lambda x: extract_style(x))
test.to_csv("test.csv", escapechar='\\')

dfdev['content_tfidf'] = dfdev['text'].parallel_apply(lambda x: process(x))
dfdev[["avg_len", "len_text", "len_words", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9",  "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]] = dfdev['text'].parallel_apply(lambda x: extract_style(x))
dfdev.to_csv("full_dev.csv")

dfdev['content_tfidf'] = dfdev['text'].parallel_apply(lambda x: process(x))
dfdev[["avg_len", "len_text", "len_words", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9",  "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]] = dfdev['text'].parallel_apply(lambda x: extract_style(x))
dfdev.to_csv("full_dev.csv")



df1 = pd.read_csv("/content/drive/MyDrive/SubtaskA/full_enron.csv")

df2 = pd.read_csv("/content/drive/MyDrive/SubtaskA/full_dev.csv")

test = pd.read_csv("/content/drive/MyDrive/SubtaskA/test.csv")

test.head(5)

test = pd.read_json('/content/drive/MyDrive/SubtaskA/subtaskA_monolingual.jsonl', lines=True)

test

test = pd.concat([test.set_index('id'), df_test_labels.set_index('id')], axis=1, join='inner').reset_index()

test= test.set_index('id').drop(columns=['Unnamed: 0'])

test.head()

df1.head()

len(df1)

len(test)

df['content_tfidf'].head(5)

len(df)

print(df1.tail())

df1 = pd.read_csv("/content/drive/MyDrive/SubtaskB/dfbtrain_enron.csv")

df2 = pd.read_csv("/content/drive/MyDrive/SubtaskB/dfbdev_enron.csv")

from sklearn.linear_model import LogisticRegression
# Evaluation
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
import os
import pickle


print("#####")
print("Training style classifier")
# Ensure that the subtaskA folder exists in your Google Drive
# Specify the path to your drive folder
drive_folder = '/content/drive/MyDrive/'

subtaskA_folder = os.path.join(drive_folder, 'SubtaskB')

X_style_train = df1[["avg_len", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9", "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]]
X_style_test = df2[["avg_len", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9", "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]]

#clf = xgb.XGBClassifier().fit(X_style_train, nlp_train['Target'])
#clf = LogisticRegression(random_state=0).fit(X_style_train, df1['label'])
clf = LogisticRegression(random_state=0, multi_class='auto', max_iter=1000).fit(X_style_train, df1['label'])

y_pred = clf.predict(X_style_test)
y_proba1 = clf.predict_proba(X_style_test)
y_proba_train1 = clf.predict_proba(X_style_train)
score_style = accuracy_score(df2['label'], y_pred)
f1_style = f1_score(df2['label'], y_pred, average="macro")

print("Training done, accuracy is : ", score_style)
print("Training done, f1-score is : ", f1_style)

# Save predicted probabilities for test set in the subtaskA folder in your Google Drive
with open(os.path.join(subtaskA_folder, 'dev_set_probabilities1.pkl'), 'wb') as f:
    pickle.dump(y_proba1, f)

# Save predicted probabilities for training set in the subtaskA folder in your Google Drive
with open(os.path.join(subtaskA_folder, 'training_set_probabilities1.pkl'), 'wb') as f:
    pickle.dump(y_proba_train1, f)

import pickle
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
import os

print("#####")
print("Training style classifier")
# Ensure that the subtaskA folder exists in your Google Drive
# Specify the path to your drive folder
drive_folder = '/content/drive/MyDrive/'

subtaskA_folder = os.path.join(drive_folder, 'SubtaskA')

X_style_train = df1[["avg_len", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9", "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]]
X_style_test = test[["avg_len", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9", "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]]

#clf = xgb.XGBClassifier().fit(X_style_train, nlp_train['Target'])
clf = LogisticRegression(random_state=0).fit(X_style_train, df1['label'])
y_pred = clf.predict(X_style_test)
y_proba1 = clf.predict_proba(X_style_test)
y_proba_train1 = clf.predict_proba(X_style_train)
# score_style = accuracy_score(test['label'], y_pred)
# f1_style = f1_score(test['label'], y_pred, average="macro")

# print("Training done, accuracy is : ", score_style)
# print("Training done, f1-score is : ", f1_style)

# Save predicted probabilities for test set in the subtaskA folder in your Google Drive
with open(os.path.join(subtaskA_folder, 'test_set_probabilities1.pkl'), 'wb') as f:
    pickle.dump(y_proba1, f)

# Save predicted probabilities for training set in the subtaskA folder in your Google Drive
with open(os.path.join(subtaskA_folder, 'training_set_probabilities1.pkl'), 'wb') as f:
    pickle.dump(y_proba_train1, f)

#  # Style-based classifier

# print("#####")
# print("Training style classifier")

# X_style_train = df1[["avg_len", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9", "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]]
# X_style_test = df2[["avg_len", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9", "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]]

# #clf = xgb.XGBClassifier().fit(X_style_train, nlp_train['Target'])
# clf = LogisticRegression(random_state=0).fit(X_style_train, df1['label'])
# y_pred = clf.predict(X_style_test)
# y_proba = clf.predict_proba(X_style_test)
# y_proba_train = clf.predict_proba(X_style_train)
# score_style = accuracy_score(df2['label'], y_pred)
# f1_style = f1_score(df2['label'], y_pred, average="macro")

# print("Training done, accuracy is : ", score_style)
# print("Training done, f1-score is : ", f1_style)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
import os
import pickle

print("#####")
print("Training style classifier")

X_style_train = df1[["avg_len", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9", "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]]
X_style_test = test[["avg_len", "num_short_w", "per_digit", "per_cap", "f_a", "f_b", "f_c", "f_d", "f_e", "f_f", "f_g", "f_h", "f_i", "f_j", "f_k", "f_l", "f_m", "f_n", "f_o", "f_p", "f_q", "f_r", "f_s", "f_t", "f_u", "f_v", "f_w", "f_x", "f_y", "f_z", "f_0", "f_1", "f_2", "f_3", "f_4", "f_5", "f_6", "f_7", "f_8", "f_9", "f_e_0", "f_e_1", "f_e_2", "f_e_3", "f_e_4", "f_e_5", "f_e_6", "f_e_7", "f_e_8", "f_e_9", "f_e_10", "f_e_11", "richness"]]

#clf = xgb.XGBClassifier().fit(X_style_train, nlp_train['Target'])
clf = LogisticRegression(random_state=0).fit(X_style_train, df1['label'])
y_pred = clf.predict(X_style_test)
y_proba = clf.predict_proba(X_style_test)
y_proba_train = clf.predict_proba(X_style_train)
score_style = accuracy_score(test['label'], y_pred)
f1_style = f1_score(test['label'], y_pred, average="macro")

print("Training done, accuracy is : ", score_style)
print("Training done, f1-score is : ", f1_style)

print(df1['label'])

len(df1)

len(df2)

print(text[:500])



print(df1['content_tfidf'].values)

print(text[0])

import nltk
nltk.download('punkt')

#Hybrid Features Extraction:
import nltk
from nltk.util import ngrams
from collections import Counter
import heapq

def return_best_bi_grams(text):
      bigrams = ngrams(text,2)

      data = dict(Counter(bigrams))
      list_ngrams = heapq.nlargest(100, data.keys(), key=lambda k: data[k])
      return list_ngrams

def return_best_tri_grams(text):
      trigrams = ngrams(text,3)

      data = dict(Counter(trigrams))
      list_ngrams = heapq.nlargest(100, data.keys(), key=lambda k: data[k])
      return list_ngrams

def find_freq_n_gram_in_txt(text, list_bigram, list_trigram):

      to_ret = []

      num_bigrams = len(Counter(zip(text,text[1:])))
      num_trigrams = len(Counter(zip(text,text[1:],text[2:])))

      for n_gram in list_bigram:
          to_ret.append(text.count(''.join(n_gram))/num_bigrams)

      for n_gram in list_trigram:
          to_ret.append(text.count(''.join(n_gram))/num_trigrams)

      return to_ret

text = " ".join(df1['text'].values)
list_bigram = return_best_bi_grams(text)
list_trigram = return_best_tri_grams(text)

text = "I am Surbhi"
num_bigrams = len(Counter(zip(text,text[1:])))
num_trigrams = len(Counter(zip(text,text[1:],text[2:])))
to_ret = []

for n_gram in list_bigram:
        print(n_gram)
        print(''.join(n_gram))
        print(text.count(''.join(n_gram)))
        to_ret.append(text.count(''.join(n_gram))/num_bigrams)

for n_gram in list_trigram:
          to_ret.append(text.count(''.join(n_gram))/num_trigrams)
print(to_ret)
print(num_bigrams)
print(num_trigrams)

import pickle

# Save list_bigram and list_trigram to a file
with open('bigram_list.pkl', 'wb') as file:
    pickle.dump(list_bigram, file)

with open('trigram_list.pkl', 'wb') as file:
    pickle.dump(list_trigram, file)

import pickle

# Save list_bigram to Google Drive
with open('/content/drive/MyDrive/SubtaskB/bigram_list.pkl', 'wb') as file:
    pickle.dump(list_bigram, file)

# Save list_trigram to Google Drive
with open('/content/drive/MyDrive/SubtaskB/trigram_list.pkl', 'wb') as file:
    pickle.dump(list_trigram, file)

import pickle

# Load the saved lists
with open('/content/drive/MyDrive/SubtaskA/bigram_list.pkl', 'rb') as file:
    list_bigram = pickle.load(file)

with open('/content/drive/MyDrive/SubtaskA/trigram_list.pkl', 'rb') as file:
    list_trigram = pickle.load(file)

print(list_bigram)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
import numpy as np


# Character N-gram only
def tokenize_text(text):
    return nltk.word_tokenize(text)

print("#####")
print("Character N-gram")
feats_train = df1['text'].apply(lambda x: find_freq_n_gram_in_txt(x, list_bigram, list_trigram)).values
feats_test = df2['text'].apply(lambda x: find_freq_n_gram_in_txt(x, list_bigram, list_trigram)).values

feats_train = pd.DataFrame(feats_train)[0].apply(lambda x: pd.Series(x))
feats_test = pd.DataFrame(feats_test)[0].apply(lambda x: pd.Series(x))

#clf_char = xgb.XGBClassifier().fit(feats_train, nlp_train['Target'])
clf_char = LogisticRegression(random_state=0).fit(feats_train, df1['label'])
y_pred = clf_char.predict(feats_test)
y_proba = clf_char.predict_proba(feats_test)
y_proba_train = clf_char.predict_proba(feats_train)

# Assuming y_proba and y_proba_train are NumPy arrays
np.save('/content/drive/MyDrive/SubtaskB/y_proba.npy1', y_proba)
np.save('/content/drive/MyDrive/SubtaskB/y_proba_train1.npy', y_proba_train)


score_char = accuracy_score(df2['label'], y_pred)
f1_char = f1_score(df2['label'], y_pred, average="macro")

print("Training done, accuracy is : ", score_char)
print("Training done, f1-score is : ", f1_char)

from sklearn.metrics import precision_score, recall_score, roc_auc_score

# Calculate precision
precision = precision_score(df2['label'], y_pred, average="macro")

# Calculate recall
recall = recall_score(df2['label'], y_pred, average="macro")

# Calculate ROC-AUC score
roc_auc = roc_auc_score(df2['label'], y_proba[:, 1])

print("Precision:", precision)
print("Recall:", recall)
print("ROC-AUC Score:", roc_auc)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
import numpy as np


# Character N-gram only
def tokenize_text(text):
    return nltk.word_tokenize(text)

print("#####")
print("Character N-gram")
feats_train = df1['text'].apply(lambda x: find_freq_n_gram_in_txt(x, list_bigram, list_trigram)).values
feats_test = test['text'].apply(lambda x: find_freq_n_gram_in_txt(x, list_bigram, list_trigram)).values

feats_train = pd.DataFrame(feats_train)[0].apply(lambda x: pd.Series(x))
feats_test = pd.DataFrame(feats_test)[0].apply(lambda x: pd.Series(x))

#clf_char = xgb.XGBClassifier().fit(feats_train, nlp_train['Target'])
clf_char = LogisticRegression(random_state=0).fit(feats_train, df1['label'])
y_pred = clf_char.predict(feats_test)
y_proba = clf_char.predict_proba(feats_test)
y_proba_train = clf_char.predict_proba(feats_train)

# Assuming y_proba and y_proba_train are NumPy arrays
np.save('/content/drive/MyDrive/SubtaskA/y_proba_test.npy', y_proba)
np.save('/content/drive/MyDrive/SubtaskA/y_proba_train11.npy', y_proba_train)


score_char = accuracy_score(test['label'], y_pred)
f1_char = f1_score(test['label'], y_pred, average="macro")

print("Training done, accuracy is : ", score_char)
print("Training done, f1-score is : ", f1_char)

print(feats_train.iloc[0])

print("Training BERT")

#if source == "blog":
#model = ClassificationModel('bert', 'bert-base-cased', limit, config, model_long, tokenizer, args={'reprocess_input_data': True, 'overwrite_output_dir': True}, use_cuda=True)
#else:
model = ClassificationModel('bert', 'bert-base-cased', num_labels=limit, args={'reprocess_input_data': True, 'overwrite_output_dir': True,  'num_train_epochs' : 15}, use_cuda=True)
model.train_model(df1[['cleaned_text', 'label']])

predictions, raw_outputs = model.predict(list(df2['cleaned_text']))
score_bert = accuracy_score(predictions, df2['label'])
f1_bert = f1_score(predictions, df2['label'], average="macro")
predictions, raw_out_train = model.predict(list(df2['cleaned_text']))

print("Training done, accuracy is : ", score_bert)
print("Training done, f1-score is : ", f1_bert)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
import numpy as np


# Character N-gram only
def tokenize_text(text):
    return nltk.word_tokenize(text)

print("#####")
print("Character N-gram")
feats_train = df1['text'].apply(lambda x: find_freq_n_gram_in_txt(x, list_bigram, list_trigram)).values
feats_test = test['text'].apply(lambda x: find_freq_n_gram_in_txt(x, list_bigram, list_trigram)).values

feats_train = pd.DataFrame(feats_train)[0].apply(lambda x: pd.Series(x))
feats_test = pd.DataFrame(feats_test)[0].apply(lambda x: pd.Series(x))

#clf_char = xgb.XGBClassifier().fit(feats_train, nlp_train['Target'])
clf_char = LogisticRegression(random_state=0).fit(feats_train, df1['label'])
y_pred = clf_char.predict(feats_test)
y_proba = clf_char.predict_proba(feats_test)
y_proba_train = clf_char.predict_proba(feats_train)

# Assuming y_proba and y_proba_train are NumPy arrays
np.save('/content/drive/MyDrive/SubtaskB/y_proba_test.npy1', y_proba)
np.save('/content/drive/MyDrive/SubtaskB/y_proba_train11.npy', y_proba_train)


score_char = accuracy_score(test['label'], y_pred)
f1_char = f1_score(test['label'], y_pred, average="macro")

print("Training done, accuracy is : ", score_char)
print("Training done, f1-score is : ", f1_char)

print("Character N-gram")
feats_train = df1['text'].apply(lambda x: find_freq_n_gram_in_txt(x, list_bigram, list_trigram)).values
feats_test = test['text'].apply(lambda x: find_freq_n_gram_in_txt(x, list_bigram, list_trigram)).values

# Convert the lists to DataFrames
feats_train = pd.DataFrame(feats_train)
feats_test = pd.DataFrame(feats_test)

clf_char = LogisticRegression(random_state=0).fit(feats_train, df1['label'])
y_pred = clf_char.predict(feats_test)
y_proba = clf_char.predict_proba(feats_test)
y_proba_train = clf_char.predict_proba(feats_train)

# Save the probability arrays
np.save('/content/drive/MyDrive/SubtaskB/y_proba_test.npy1', y_proba)
np.save('/content/drive/MyDrive/SubtaskB/y_proba_train11.npy', y_proba_train)

score_char = accuracy_score(test['label'], y_pred)
f1_char = f1_score(test['label'], y_pred, average="macro")

print("Training done, accuracy is : ", score_char)
print("Training done, f1-score is : ", f1_char)

# Reset the index of the DataFrame
df1.reset_index(drop=True, inplace=True)
test.reset_index(drop=True, inplace=True)

print("Character N-gram")
feats_train = df1['text'].apply(lambda x: find_freq_n_gram_in_txt(x, list_bigram, list_trigram)).values
feats_test = test['text'].apply(lambda x: find_freq_n_gram_in_txt(x, list_bigram, list_trigram)).values

# Convert the lists to DataFrames
feats_train = pd.DataFrame(feats_train)
feats_test = pd.DataFrame(feats_test)

# Train logistic regression model
clf_char = LogisticRegression(random_state=0).fit(feats_train, df1['label'])
y_pred = clf_char.predict(feats_test)
y_proba = clf_char.predict_proba(feats_test)
y_proba_train = clf_char.predict_proba(feats_train)

# Save the probability arrays
np.save('/content/drive/MyDrive/SubtaskB/y_proba_test.npy1', y_proba)
np.save('/content/drive/MyDrive/SubtaskB/y_proba_train11.npy', y_proba_train)

# Evaluate model performance
score_char = accuracy_score(test['label'], y_pred)
f1_char = f1_score(test['label'], y_pred, average="macro")

print("Training done, accuracy is : ", score_char)
print("Training done, f1-score is : ", f1_char)

from collections import Counter

def find_freq_n_gram_in_txt(text, list_bigram, list_trigram):
    to_ret = []

    num_bigrams = len(Counter(zip(text, text[1:])))
    num_trigrams = len(Counter(zip(text, text[1:], text[2:])))

    for n_gram in list_bigram:
        to_ret.append(text.count(''.join(n_gram)) / num_bigrams)

    if num_trigrams > 0:  # Check if num_trigrams is greater than zero to avoid division by zero
        for n_gram in list_trigram:
            to_ret.append(text.count(''.join(n_gram)) / num_trigrams)

    return to_ret

# Extract text values as a list
texts_train = df1['text'].tolist()
# Extract text values as a list
texts_test = test['text'].values.tolist()


# Apply the function to each text individually
feats_train = [find_freq_n_gram_in_txt(text, list_bigram, list_trigram) for text in texts_train]
feats_test = [find_freq_n_gram_in_txt(text, list_bigram, list_trigram) for text in texts_test]

# Convert the lists to DataFrames
feats_train = pd.DataFrame(feats_train)
feats_test = pd.DataFrame(feats_test)
print(feats_test)

# # Train logistic regression model
# clf_char = LogisticRegression(random_state=0).fit(feats_train, df1['label'])
# y_pred = clf_char.predict(feats_test)
# y_proba = clf_char.predict_proba(feats_test)
# y_proba_train = clf_char.predict_proba(feats_train)

# # Save the probability arrays
# np.save('/content/drive/MyDrive/SubtaskB/y_proba_test.npy1', y_proba)
# np.save('/content/drive/MyDrive/SubtaskB/y_proba_train11.npy', y_proba_train)

# # Evaluate model performance
# score_char = accuracy_score(test['label'], y_pred)
# f1_char = f1_score(test['label'], y_pred, average="macro")

# print("Training done, accuracy is : ", score_char)
# print("Training done, f1-score is : ", f1_char)

df1.head()

from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch

# Assuming `model` is your fine-tuned RoBERTa model
# Assuming `tokenizer` is the corresponding tokenizer

# Load your data
# Assuming `nlp_train` and `nlp_dev` are your training and development datasets

# Set the model to evaluation mode
model = RobertaForSequenceClassification.from_pretrained('/content/drive/My Drive/fine_tuned_model/')
tokenizer = RobertaTokenizer.from_pretrained('/content/drive/My Drive/fine_tuned_model/')


model.eval()
# Function to get logits
def get_logits(texts):
    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        logits = model(**inputs).logits
    return logits

# Get logits for training and dev data
train_logits = get_logits(list(df1['content_tfidf']))
dev_logits = get_logits(list(df2['content_tfidf']))

# Tokenize and convert the data to tensors
# train_inputs = tokenizer(list(df1['content_tfidf']), return_tensors='pt', padding=True, truncation=True, max_length=512)
# dev_inputs = tokenizer(list(df2['content_tfidf']), return_tensors='pt', padding=True, truncation=True, max_length=512)


# # Forward pass to get the logits
# with torch.no_grad():
#     train_logits = model(**train_inputs).logits
#     dev_logits = model(**dev_inputs).logits

# Save the logits to files
torch.save(train_logits, '/content/drive/MyDrive/SubtaskA/train_logits.pt')
torch.save(dev_logits, '/content/drive/MyDrive/SubtaskA/dev_logits.pt')

from math import ceil
# Set the model to evaluation mode
from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch
model = RobertaForSequenceClassification.from_pretrained('/content/drive/My Drive/fine_tuned_model/')
tokenizer = RobertaTokenizer.from_pretrained('/content/drive/My Drive/fine_tuned_model/')



def process_in_batches(texts, batch_size):
    num_batches = ceil(len(texts) / batch_size)
    all_logits = []
    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = (i + 1) * batch_size
        batch_texts = texts[start_idx:end_idx]

        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)
        with torch.no_grad():
            logits = model(**inputs).logits
        all_logits.append(logits)

    return torch.cat(all_logits, dim=0)

# Assuming df1['content_tfidf'] and df2['content_tfidf'] are lists of texts
batch_size = 4  # Adjust this based on your available memory

# Get logits for training data in batches
train_logits = process_in_batches(list(df1['content_tfidf']), batch_size)

# Get logits for dev data in batches
dev_logits = process_in_batches(list(df2['content_tfidf']), batch_size)

# Save the logits to files
torch.save(train_logits, '/content/drive/MyDrive/SubtaskA/train_logits.pt')
torch.save(dev_logits, '/content/drive/MyDrive/SubtaskA/dev_logits.pt')

# Model Combination(BERT+STYLE)

print("#####")
print("Model combination")

feat_for_BERT_LR_train = np.concatenate([raw_out_train, y_proba_train], axis=1)
feat_for_BERT_LR_test = np.concatenate([raw_outputs, y_proba], axis=1)

clf = LogisticRegression(random_state=0).fit(feat_for_BERT_LR_train, df1['label'])
#clf = xgb.XGBClassifier().fit(feat_for_BERT_LR_train, nlp_train['Target'])

y_pred = clf.predict(feat_for_BERT_LR_test)
score_comb = accuracy_score(df2['label'], y_pred)
f1_comb = f1_score(df2['label'], y_pred, average="macro")

print("Training done, accuracy is : ", score_comb)
print("Training done, f1-score is : ", f1_comb)

# BERT + Style + Char N-gram

print("#####")
print("BERT + Style + Char N-gram")

feat_for_BERT_full_train = np.concatenate([feat_for_BERT_LR_train, y_proba_train], axis=1)
feat_for_BERT_full_test = np.concatenate([feat_for_BERT_LR_test, y_proba], axis=1)

#clf = xgb.XGBClassifier().fit(feat_for_BERT_full_train, nlp_train['Target'])
clf = LogisticRegression(random_state=0).fit(feat_for_BERT_full_train, df1['label'])

y_pred = clf.predict(feat_for_BERT_full_test)
score_comb_fin = accuracy_score(df2['label'], y_pred)
f1_comb_fin = f1_score(df2['label'], y_pred, average="macro")
print("Training done, accuracy is : ", score_comb_fin)
print("Training done, f1-score is : ", f1_comb_fin)

import os
import torch
import torch.nn.functional as F
import pickle

# Specify the path to your drive folder
drive_folder = '/content/drive/MyDrive/'

# Load predicted probabilities for test set
test_file_path = os.path.join(drive_folder, 'SubtaskA', 'logitslisttestnotrefined.pkl')
with open(test_file_path, 'rb') as f:
    raw_outputs1 = pickle.load(f)

# Convert raw_outputs1 to a PyTorch tensor
raw_outputs1 = torch.tensor(raw_outputs1)

raw_outputs = F.softmax(raw_outputs1, dim=1)

# Load predicted probabilities for training set
train_file_path = os.path.join(drive_folder, 'SubtaskA', 'train_logitsrefined.pkl')
with open(train_file_path, 'rb') as f:
    raw_out_train1 = pickle.load(f)

# Convert raw_out_train1 to a PyTorch tensor
raw_out_train1 = torch.tensor(raw_out_train1)

raw_out_train = F.softmax(raw_out_train1, dim=1)

# Load the saved arrays
import numpy as np
y_proba = np.load('/content/drive/MyDrive/SubtaskA/y_proba_test.npy')
y_proba_train = np.load('/content/drive/MyDrive/SubtaskA/y_proba_train11.npy')

#Model Combination(BERT+STYLE)
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.preprocessing import StandardScaler
print("#####")
print("Model combination")

feat_for_BERT_LR_train = np.concatenate([raw_out_train, y_proba_train], axis=1)
feat_for_BERT_LR_test = np.concatenate([raw_outputs, y_proba], axis=1)
# # Scale the features
# scaler = StandardScaler()
# scaled_feat_for_BERT_LR_train = scaler.fit_transform(feat_for_BERT_LR_train)
# scaled_feat_for_BERT_LR_test = scaler.transform(feat_for_BERT_LR_test)


clf = LogisticRegression(random_state=0).fit(feat_for_BERT_LR_train, df1['label'])
#clf = xgb.XGBClassifier().fit(feat_for_BERT_LR_train, nlp_train['Target'])

y_pred = clf.predict(feat_for_BERT_LR_test)


# # Assuming you have predictions in y_pred
# # Create a list of dictionaries for the JSONL file
# predictions_list = [{"id": text_id, "label": int(predicted_label)} for text_id, predicted_label in zip(df2['id'], y_pred)]

# # Specify the path for the JSONL file
# jsonl_file_path = '/content/drive/MyDrive/SubtaskB/predictionsbertsylenotrefinedTESTano.jsonl'

# # Write the list of dictionaries to the JSONL file
# with open(jsonl_file_path, 'w') as jsonl_file:
#     for entry in predictions_list:
#         jsonl_file.write(json.dumps(entry) + '\n')

# print("Prediction file saved at:", jsonl_file_path)
score_comb = accuracy_score(test['label'], y_pred)
f1_comb = f1_score(test['label'], y_pred, average="macro")

print("Training done, accuracy is : ", score_comb)
print("Training done, f1-score is : ", f1_comb)

#BERT + Style + Char N-gram
import json

print("#####")
print("BERT + Style + Char N-gram")

feat_for_BERT_full_train = np.concatenate([feat_for_BERT_LR_train, y_proba_train], axis=1)
feat_for_BERT_full_test = np.concatenate([feat_for_BERT_LR_test, y_proba], axis=1)

#clf = xgb.XGBClassifier().fit(feat_for_BERT_full_train, nlp_train['Target'])
clf = LogisticRegression(random_state=0).fit(feat_for_BERT_full_train, df1['label'])

y_pred = clf.predict(feat_for_BERT_full_test)

# # Assuming you have predictions in y_pred
# # Create a list of dictionaries for the JSONL file
# predictions_list = [{"id": text_id, "label": int(predicted_label)} for text_id, predicted_label in zip(df2['id'], y_pred)]

# # Specify the path for the JSONL file
# jsonl_file_path = '/content/drive/MyDrive/SubtaskB/predictionsbertstylecharnotrefinedTESTano.jsonl'

# # Write the list of dictionaries to the JSONL file
# with open(jsonl_file_path, 'w') as jsonl_file:
#     for entry in predictions_list:
#         jsonl_file.write(json.dumps(entry) + '\n')

# print("Prediction file saved at:", jsonl_file_path)
score_comb_fin = accuracy_score(test['label'], y_pred)
f1_comb_fin = f1_score(test['label'], y_pred, average="macro")
print("Training done, accuracy is : ", score_comb_fin)
print("Training done, f1-score is : ", f1_comb_fin)